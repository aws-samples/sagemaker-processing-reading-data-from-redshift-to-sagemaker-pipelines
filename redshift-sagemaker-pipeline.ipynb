{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use data from Amazon RedShift in a scheduled training pipeline with Amazon SageMaker\n",
    "\n",
    "Customers in many different domains tend to work with multiple sources for their data: object-based storage like Amazon S3, relational databases like Amazon RDS, data warehouses like Amazon Redshift. Machine Learning practitioners are often driven to work with objects and files instead of databases and tables from the different frameworks they work with, and prefer local copies of such files in order to reduce to the minimum the latency of accessing them.\n",
    "\n",
    "Nevertheless, ML engineers and Data Scientists might be required to directly extract with SQL-like queries data from data warehouses to obtain the datasets that they can use for training their models.\n",
    "\n",
    "In this blog post, we use Amazon SageMaker Processing API to run a query against a RedShift cluster, create CSV files, and perform distributed processing. Then, we train a simple model to predict the total sales for new events, and build a pipeline with Amazon SageMaker Pipelines to be able to schedule it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This blog post uses the sample data that is available when creating a Free Tier cluster in Amazon RedShift. Here, we take into account that your RedShift cluster has already been created and that you have attached to it an IAM role with the correct permissions. To learn how to do both these operations, check the two following links:\n",
    "\n",
    "- Create the cluster with the sample dataset - [Link](https://docs.aws.amazon.com/redshift/latest/gsg/sample-data-load.html)\n",
    "- Associate the role to the cluster - [Link](https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html#data-api-access)\n",
    "\n",
    "After this has been created, you can use your IDE of choice to open the notebooks. The content has been developed and tested using SageMaker Studio, on a `ml.t3.medium` instance. If you want to know more about SageMaker Studio, you can learn here:\n",
    "\n",
    "- What is Amazon SageMaker Studio - [Link](https://aws.amazon.com/sagemaker/studio/)\n",
    "- How to set-up you Amazon Sagemaker Studio domain - [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)\n",
    "- Cloning a repository in Sagemaker Studio - [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-git.html)\n",
    "- Changing instance type on a SageMaker Notebook - [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-run-and-manage-switch-instance-type.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: configure your cluster access, your role ARN, your output S3 URI\n",
    "\n",
    "In the following cell, you will be prompted to provide some of the information associated to your Redshift cluster and S3 path of output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = input(\"ARN of the Execution Role for this notebook:\")\n",
    "\n",
    "###### CLUSTER CONFIGURATION\n",
    "cluster_id = input(\"The name of your Redshift cluster:\")\n",
    "cluster_role_name = input(\"The name of the Role you've associated to your Redshift Cluster (not the ARN, default: myRedshiftRole):\") or \"myRedshiftRole\"\n",
    "cluster_role_arn = f'arn:aws:iam::{session.account_id()}:role/service-role/{cluster_role_name}'\n",
    "database = input(\"The database of your Redshift cluster (default: dev)\") or 'dev'\n",
    "db_user = input(\"The user of your Redshift cluster (default: awsuser)\") or 'awsuser'\n",
    "\n",
    "###### OUTPUT S3 PATH\n",
    "bucket = input(\"Your S3 bucket (leave empty for default):\") or session.default_bucket()\n",
    "key_prefix = input(\"The path where to save the output of the Redshift query in S3 (default: redshift-demo/redshift2processing/data/)\") or \"redshift-demo/redshift2processing/data/\"\n",
    "output_s3_uri = f's3://{bucket}/{key_prefix}'\n",
    "\n",
    "# Output the info\n",
    "print(f'\\n\\nCluster ID: {cluster_id}\\nRole ARN: {cluster_role_arn}\\nOutput S3 URI: {output_s3_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that Redshift cluster is set, we can now prepare our SQL query string. In this example, we plan on predicting total sales for a specific event, provided its venue, category, date and holiday information. The query is a pretty basic one but can be improved as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### QUERY STRING\n",
    "query_string = \"\"\"\n",
    "-- Find total sales for specific event, plus additional features\n",
    "SELECT sum(s.qtysold) AS total_sold, sum(s.pricepaid) AS total_paid, e.venueid, e.catid, d.caldate, d.holiday\n",
    "from sales s, event e, date d\n",
    "WHERE s.eventid = e.eventid and e.dateid = d.dateid\n",
    "GROUP BY e.venueid, e.catid, d.caldate, d.holiday\n",
    "\"\"\" # this will work on the default Free Tier Redshift cluster. Change if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from Redshift with SageMaker Processing\n",
    "\n",
    "The first step is to create a `RedshiftDatasetDefinition`. This is part of the SageMaker Python SDK and defines how you are supposed to read data from RedShift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.dataset_definition.inputs import RedshiftDatasetDefinition\n",
    "\n",
    "rdd = RedshiftDatasetDefinition(\n",
    "    cluster_id=cluster_id,\n",
    "    database=database,\n",
    "    db_user=db_user,\n",
    "    query_string=query_string,\n",
    "    cluster_role_arn=cluster_role_arn,\n",
    "    output_format='CSV',\n",
    "    output_s3_uri=output_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can define the `DatasetDefinition`. This object is responsible of defining how SageMaker Processing will use the dataset loaded from Redshift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.dataset_definition.inputs import DatasetDefinition\n",
    "\n",
    "dd = DatasetDefinition(\n",
    "    data_distribution_type='ShardedByS3Key',\n",
    "    local_path='/opt/ml/processing/input/data/',\n",
    "    redshift_dataset_definition=rdd\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use this object as input of your Processor of choice. Here, we have written a very simple SKLearn script that does a bit of cleaning of the dataset, some transformation, as well as a split of train and test dataset. You can check the code in the file `processing.py` . You can now define the `SKLearnProcessor` and pass the `dataset_definition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "skp = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type='ml.m5.large',\n",
    "    instance_count=2\n",
    ")\n",
    "skp.run(\n",
    "    code='processing/processing.py',\n",
    "    inputs=[ProcessingInput(\n",
    "        input_name='source',\n",
    "        dataset_definition=dd,\n",
    "        destination='/opt/ml/processing/input/data/',\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )],\n",
    "    outputs = [\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the outputs created by the Processing job, we can move to the training step, by the means of the `sagemaker.sklearn.SKLearn()` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearn\n",
    "\n",
    "s = SKLearn(\n",
    "    entry_point='training/script.py',\n",
    "    framework_version='0.23-1',\n",
    "    instance_type='ml.m5.large',\n",
    "    instance_count=1,\n",
    "    role=role\n",
    ")\n",
    "s.fit({\n",
    "    'train':skp.latest_job.outputs[0].destination, \n",
    "    'test':skp.latest_job.outputs[1].destination\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the pieces together in a Pipeline\n",
    "\n",
    "SageMaker Pipelines allows us to build a workflow by means of steps, each one of the referring to a specific SageMaker feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep, ProcessingInput, ProcessingOutput, \n",
    "    TrainingStep, TrainingInput, \n",
    "    CreateModelStep, CreateModelInput\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the pipeline will preprocess the data to prepare it for training. We create a `SKLearnProcessor` object similar to the one created previously, but now parameterized so we can separately track and change the job configuration as needed, for example to increase the instance type size and count to accommodate a growing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PROCESSING STEP #####\n",
    "\n",
    "# PARAMETERS\n",
    "processing_instance_type = ParameterString(name='ProcessingInstanceType', default_value='ml.m5.large')\n",
    "processing_instance_count = ParameterInteger(name='ProcessingInstanceCount', default_value=2)\n",
    "\n",
    "# PROCESSOR\n",
    "skp = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count\n",
    ")\n",
    "\n",
    "# DEFINE THE STEP\n",
    "processing_step = ProcessingStep(\n",
    "    name='ProcessingStep',\n",
    "    processor=skp,\n",
    "    code='processing/processing.py',\n",
    "    inputs=[ProcessingInput(\n",
    "        dataset_definition=dd,\n",
    "        destination='/opt/ml/processing/input/data/',\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )],\n",
    "    outputs = [\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output/test\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sets up a pipeline step for a training job. We specify an `Estimator` object, and define a `TrainingStep` to insert the training job in the pipeline with inputs from the previous SageMaker Processing step. Since the training script is pretty basic, it does not accept any hyperparameter to the model, but this can be easily changed by adapting the `ArgumentParser` in the [`script.py`](training/script.py) file and adding the hyperparameters here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAINING STEP ####\n",
    "\n",
    "# PARAMETERS\n",
    "training_instance_type = ParameterString(name='TrainingInstanceType', default_value='ml.m5.large')\n",
    "training_instance_count = ParameterInteger(name='TrainingInstanceCount', default_value=1)\n",
    "\n",
    "# ESTIMATOR\n",
    "s = SKLearn(\n",
    "    entry_point='training/script.py',\n",
    "    framework_version='0.23-1',\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "# TRAININGSTEP\n",
    "training_step = TrainingStep(\n",
    "    name='TrainingStep',\n",
    "    estimator=s,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri),\n",
    "        \"test\": TrainingInput(s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another step, we create a SageMaker `Model` object to wrap the model artifact, and associate it with the SageMaker prebuilt SKLearn inference container to potentially use later, i.e. in an inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MODEL STEP ####\n",
    "\n",
    "# PARAMETERS\n",
    "inference_instance_type = ParameterString(name='InferenceInstanceType', default_value='ml.m5.large')\n",
    "\n",
    "# MODEL\n",
    "model = SKLearnModel(\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=role,\n",
    "    entry_point=\"training/script.py\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# MODELSTEP\n",
    "model_step = CreateModelStep(\n",
    "    name=\"Model\",\n",
    "    model=model,\n",
    "    inputs=CreateModelInput(instance_type=inference_instance_type)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of the pipeline steps now defined, we can define the pipeline itself as a `Pipeline` object comprising a series of those steps. `ParallelStep` and `Condition` steps also are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PIPELINE ####\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name = 'Redshift2Pipeline',\n",
    "    parameters = [\n",
    "        processing_instance_type, processing_instance_count,\n",
    "        training_instance_type, training_instance_count,\n",
    "        inference_instance_type\n",
    "    ],\n",
    "    steps = [\n",
    "        processing_step, \n",
    "        training_step,\n",
    "        model_step\n",
    "    ]\n",
    ")\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upserting its definition, we can start the pipeline with the Pipeline object's `start()` method, and wait for the end of its execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pipeline started executing, you can view the pipeline run.\n",
    "\n",
    "To view them, choose the SageMakers Components and registries button. On the Components and registires drop down, select Pipelines. Click the `Redshift2Pipeline` pipeline, and then double click on the execution. Now you can see the pipeline executing. Click on each step to see additional details such as the output, logs and additional information. Typically this pipeline should take about 10 minutes to complete. \n",
    "\n",
    "You can check its outputs by calling the `describe()` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Congratulations! You have now created a complete training pipeline for your Redshift data! \n",
    "\n",
    "A few things you can do now:\n",
    "\n",
    "- schedule the execution of this pipeline with Amazon Eventbridge Rules - [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/automating-sagemaker-with-eventbridge.html)\n",
    "- create a new scheduled SageMaker Pipeline for inference with the [TransformStep](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-transform)\n",
    "- use the model you've created to update an existing real-time endpoint [manually](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpoint.html) or as part of a [SageMaker Project](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-whatis.html)\n",
    "\n",
    "If you want some additional notebooks to play with you can check:\n",
    "\n",
    "- how to use the RedShift Data API from within a SageMaker Notebook - [Link](extra-content/data-api-discovery.ipynb)\n",
    "- how to integrate the Redshift Data API in a Lambda function to have more granular control, and add this step to a SageMaker Pipeline - [Link](extra-content/pipeline.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
